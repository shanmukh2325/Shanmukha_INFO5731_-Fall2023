{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanmukh2325/Shanmukha_INFO5731_-Fall2023/blob/main/Bollavaram_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "An interesting text classification task in healthcare could be the classification of medical research articles into categories such as \"Clinical Trials,\" \"Drug Discovery,\" \"Epidemiology,\" \"Genomic Research,\" and \"Disease Diagnosis.\" This task would assist researchers in quickly identifying relevant articles for their specific research interests.\n",
        "\n",
        "Here are five types of features that might be useful for building a machine learning model for this task:\n",
        "\n",
        "Bag of Words (BoW) Features: BoW features represent the frequency of each word in the text. These features can help capture the most common terms associated with each research category.\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) Features: TF-IDF measures the importance of a word in a document relative to its importance across the entire corpus. These features can highlight words that are particularly relevant to a specific research category.\n",
        "\n",
        "N-grams Features: N-grams represent sequences of adjacent words or characters. Extracting n-grams can capture important phrases or patterns in the text that may be indicative of the research category.\n",
        "\n",
        "Word Embeddings: Word embeddings, such as Word2Vec or GloVe, can convert words into dense vector representations. These embeddings capture semantic relationships between words and can be used to understand the context of terms in the text.\n",
        "\n",
        "Topic Modeling Features: Topic modeling techniques like Latent Dirichlet Allocation (LDA) can identify the underlying topics in a document. These features can help categorize research articles based on the topics they cover\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00abec28-25a0-4e50-c1fb-f65e72acc042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Features:\n",
            "[[0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0]\n",
            " [0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1]\n",
            " [1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0]]\n",
            "\n",
            "TF-IDF Features:\n",
            "[[0.         0.         0.         0.         0.         0.33942996\n",
            "  0.         0.         0.         0.         0.26761048 0.33942996\n",
            "  0.         0.         0.         0.33942996 0.         0.21665375\n",
            "  0.         0.21665375 0.26761048 0.         0.33942996 0.\n",
            "  0.         0.         0.         0.         0.26761048 0.33942996\n",
            "  0.33942996 0.        ]\n",
            " [0.         0.         0.         0.36222393 0.36222393 0.\n",
            "  0.36222393 0.         0.         0.         0.         0.\n",
            "  0.         0.2855815  0.36222393 0.         0.36222393 0.\n",
            "  0.         0.         0.         0.36222393 0.         0.\n",
            "  0.36222393 0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.33226431 0.         0.         0.\n",
            "  0.         0.         0.         0.33226431 0.         0.\n",
            "  0.33226431 0.         0.         0.         0.         0.21208001\n",
            "  0.33226431 0.21208001 0.261961   0.         0.         0.\n",
            "  0.         0.         0.33226431 0.33226431 0.261961   0.\n",
            "  0.         0.33226431]\n",
            " [0.35227855 0.35227855 0.         0.         0.         0.\n",
            "  0.         0.35227855 0.35227855 0.         0.27774046 0.\n",
            "  0.         0.27774046 0.         0.         0.         0.22485484\n",
            "  0.         0.22485484 0.         0.         0.         0.35227855\n",
            "  0.         0.35227855 0.         0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "\n",
            "N-grams Features:\n",
            "[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            "  1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0]\n",
            " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            "  0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1]\n",
            " [1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Topic Modeling Features (LDA):\n",
            "[[0.05494261 0.94505739]\n",
            " [0.05955745 0.94044255]\n",
            " [0.95260412 0.04739588]\n",
            " [0.05112605 0.94887395]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Sample healthcare research articles\n",
        "documents = [\n",
        "    \"A randomized clinical trial on the effectiveness of drug X in treating hypertension.\",\n",
        "    \"Genomic research identifies potential biomarkers for cancer diagnosis.\",\n",
        "    \"Epidemiological study on the spread of infectious diseases in urban areas.\",\n",
        "    \"A review of recent advances in drug discovery for Alzheimer's disease.\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "nltk.download('punkt')\n",
        "tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Bag of Words (BoW) features\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# TF-IDF features\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_features = vectorizer_tfidf.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# N-grams features (bi-grams and tri-grams)\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 3))\n",
        "ngrams_features = vectorizer_ngrams.fit_transform([\" \".join(doc) for doc in tokenized_documents])\n",
        "\n",
        "# Topic modeling features using LDA\n",
        "lda = LatentDirichletAllocation(n_components=2)\n",
        "lda_features = lda.fit_transform(bow_features)\n",
        "\n",
        "# Word embeddings (using pre-trained Word2Vec or GloVe embeddings)\n",
        "# You would typically load pre-trained embeddings and transform the text into vectors.\n",
        "\n",
        "# Display the extracted features\n",
        "print(\"BoW Features:\")\n",
        "print(bow_features.toarray())\n",
        "\n",
        "print(\"\\nTF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "\n",
        "print(\"\\nN-grams Features:\")\n",
        "print(ngrams_features.toarray())\n",
        "\n",
        "print(\"\\nTopic Modeling Features (LDA):\")\n",
        "print(lda_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88a7d67-2378-4d20-8625-182156865fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features in Descending Order of Importance:\n",
            "['advances', 'treating', 'study', 'spread', 'review', 'research', 'recent', 'randomized', 'potential', 'infectious', 'identifies', 'trial', 'genomic', 'hypertension', 'epidemiological', 'alzheimer', 'areas', 'biomarkers', 'cancer', 'clinical', 'diagnosis', 'discovery', 'diseases', 'effectiveness', 'disease', 'urban', 'for', 'on', 'drug', 'the', 'of', 'in']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define the target labels (research categories)\n",
        "labels = [\"Clinical Trials\", \"Genomic Research\", \"Epidemiology\", \"Drug Discovery\"]\n",
        "\n",
        "# Assuming you have a target label for each document, e.g., [0, 1, 2, 3]\n",
        "\n",
        "# Feature selection using chi-squared statistic\n",
        "k_best = SelectKBest(score_func=chi2, k='all')\n",
        "selected_features = k_best.fit_transform(bow_features, labels)\n",
        "\n",
        "# Get the indices of selected features in descending order of importance\n",
        "sorted_feature_indices = (-k_best.scores_).argsort()\n",
        "\n",
        "# List the features in descending order of importance\n",
        "sorted_features = [vectorizer_bow.get_feature_names_out()[i] for i in sorted_feature_indices]\n",
        "\n",
        "print(\"Top Features in Descending Order of Importance:\")\n",
        "print(sorted_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amHZ_JT0HvV6",
        "outputId": "bf68178c-9ac8-4fac-fe7f-c893cd92e73c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-kJRfyIGsM",
        "outputId": "fbf7946c-a8b3-4a34-d01e-cf700e4610cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c2e501-b838-4a57-bb0f-a674559837b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1: Similarity = 0.8249\n",
            "A randomized clinical trial on the effectiveness of drug X in treating hypertension.\n",
            "\n",
            "Rank 2: Similarity = 0.8074\n",
            "A review of recent advances in drug discovery for Alzheimer's disease.\n",
            "\n",
            "Rank 3: Similarity = 0.7311\n",
            "Genomic research identifies potential biomarkers for cancer diagnosis.\n",
            "\n",
            "Rank 4: Similarity = 0.6817\n",
            "Epidemiological study on the spread of infectious diseases in urban areas.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "# Your query\n",
        "query = \"Clinical trial for Alzheimer's disease treatment.\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose other BERT variants as well\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Encode the query and documents\n",
        "query_encoding = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "document_encodings = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Get BERT embeddings for the query and documents\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**query_encoding).last_hidden_state.mean(dim=1)\n",
        "    document_embeddings = model(**document_encodings).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate cosine similarity between the query and documents\n",
        "similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
        "\n",
        "# Rank documents by similarity in descending order\n",
        "ranked_documents = [(document, similarity) for document, similarity in zip(documents, similarities)]\n",
        "ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked documents\n",
        "for i, (document, similarity) in enumerate(ranked_documents, start=1):\n",
        "    print(f\"Rank {i}: Similarity = {similarity:.4f}\")\n",
        "    print(document)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}