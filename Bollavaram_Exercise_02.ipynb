{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW5_oFVd9-pY"
   },
   "source": [
    "## The second In-class-exercise (09/13/2023, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAzh1U0sE5I5"
   },
   "source": [
    "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
    "Execute all the cells before your final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpgvZQdRE-HV"
   },
   "source": [
    "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QBZI-je9-pZ"
   },
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWoKpYQT9-pa"
   },
   "source": [
    "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LmNR3kw9-pa"
   },
   "outputs": [],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "\n",
    "Research Question: What are the most common topics covered in research news articles on the University of North Texas website?\n",
    "\n",
    "To answer this question, we would need to collect data on the news titles, leads, and story tags from the research news articles on the University of North Texas website.\n",
    "\n",
    "Data Needed:\n",
    "\n",
    "News Title\n",
    "News Lead\n",
    "Story Tags (if available)\n",
    "Number of Data Needed for the Analysis:\n",
    "Since this is an exploratory analysis, it's hard to specify an exact number of data samples needed. However, a larger sample size would provide more reliable insights. For this exercise, you collected up to 1000 data samples, which should be sufficient for an initial analysis.\n",
    "\n",
    "Steps for Collecting and Saving the Data:\n",
    "\n",
    "Setting Up the Environment:\n",
    "Import necessary libraries (urllib.request, BeautifulSoup, and csv).\n",
    "Initialize variables like news_count, max_news_count, max_pages, and base_url.\n",
    "Web Scraping:\n",
    "Use a loop to iterate through pages (in this case, up to 18 pages).\n",
    "For each page:\n",
    "Construct the URL.\n",
    "Send an HTTP GET request to the URL.\n",
    "Parse the HTML content using BeautifulSoup.\n",
    "Find all elements with class \"news-item\".\n",
    "Extracting Data:\n",
    "Inside the loop through news items, extract the news title, lead, and story tags (if available).\n",
    "Storing Data:\n",
    "Append the extracted data to a list (data).\n",
    "Saving to CSV:\n",
    "After collecting all data samples, save the data to a CSV file.\n",
    "Open a CSV file in write mode, create a CSV writer, write the header row, and then write the data rows.\n",
    "Print Confirmation:\n",
    "Print a message to confirm that the data has been saved to the CSV file.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlxTLRNm9-pa"
   },
   "source": [
    "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QpWOgjHi9-pa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to news_data.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Initialize variables to count news items and pages\n",
    "news_count = 0\n",
    "max_news_count = 1000  # You can adjust this as needed\n",
    "max_pages = 18  # The number of pages you want to scrape\n",
    "\n",
    "# Initialize the base URL\n",
    "base_url = \"https://research.unt.edu/news?page=\"\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each page\n",
    "for page_num in range(1, max_pages + 1):\n",
    "    # Construct the URL for the current page\n",
    "    url = base_url + str(page_num)\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "    # Find all elements with class \"news-item\"\n",
    "    news_items = soup.find_all(class_='news-item')\n",
    "\n",
    "    # Loop through the found news items and extract title, lead, and story tags\n",
    "    for item in news_items:\n",
    "        if news_count >= max_news_count:\n",
    "            break\n",
    "\n",
    "        # Extract the news title (update class name as needed)\n",
    "        news_title_element = item.find(class_='news-title')  # Replace 'news-title' with the correct class name\n",
    "        news_title = news_title_element.text.strip() if news_title_element else \"Title Not Available\"\n",
    "\n",
    "        # Extract the news lead\n",
    "        news_lead_element = item.find(class_='news-lead')\n",
    "        news_lead = news_lead_element.text.strip() if news_lead_element else \"Lead Not Available\"\n",
    "\n",
    "        # Extract the story tags, if available\n",
    "        story_tags_element = item.find_all(class_='story-tags')\n",
    "        story_tags = ', '.join([tag.text.strip() for tag in story_tags_element]) if story_tags_element else \"Story Tags Not Available\"\n",
    "\n",
    "        # Append the data to the list\n",
    "        data.append([news_title, news_lead, story_tags])\n",
    "\n",
    "        news_count += 1\n",
    "\n",
    "    # If you reached the maximum news count, exit the loop\n",
    "    if news_count >= max_news_count:\n",
    "        break\n",
    "\n",
    "# Save the data to a CSV file\n",
    "csv_filename = \"news_data.csv\"\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header row\n",
    "    csv_writer.writerow([\"News Title\", \"News Lead\", \"Story Tags\"])\n",
    "    \n",
    "    # Write the data\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "print(f\"Data saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px6wgvog9-pa"
   },
   "source": [
    "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
    "\n",
    "The following information of the article needs to be collected:\n",
    "\n",
    "(1) Title\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: attrs>=20.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium)\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.4 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "P5rjlclf9-pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n",
      "Scraped 660 articles and saved to acm_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Define the URL and headers\n",
    "base_url = \"https://dl.acm.org/action/doSearch\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Your User-Agent Header Here\",\n",
    "}\n",
    "\n",
    "# Initialize variables\n",
    "total_samples = 1000\n",
    "samples_collected = 0\n",
    "page_number = 1\n",
    "\n",
    "# Initialize a list to store the data\n",
    "article_data = []\n",
    "\n",
    "while samples_collected < total_samples:\n",
    "    # Define query parameters for the search\n",
    "    params = {\n",
    "        \"AllField\": \"data\",\n",
    "        \"expand\": \"all\",\n",
    "        \"ConceptID\": \"118230\",\n",
    "        \"pageNumber\": page_number,\n",
    "    }\n",
    "\n",
    "    # Send an HTTP GET request to the URL with query parameters\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the section with the article titles and authors\n",
    "        articles = soup.find_all('h5', class_='issue-item__title')\n",
    "\n",
    "        # Extract article titles and authors\n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            article_data.append({'Title': title})\n",
    "            samples_collected += 1\n",
    "\n",
    "        # Increment the page number for pagination\n",
    "        page_number += 1\n",
    "\n",
    "        # Break the loop if the desired number of samples is reached\n",
    "        if samples_collected >= total_samples:\n",
    "            break\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open('acm_articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(article_data)\n",
    "\n",
    "print(f\"Scraped {samples_collected} articles and saved to acm_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT3CNj_V9-pb"
   },
   "source": [
    "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
    "\n",
    "The following information needs to be collected:\n",
    "\n",
    "(1) User_name\n",
    "\n",
    "(2) Posted time\n",
    "\n",
    "(3) Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FymVNKVi9-pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected and saved 1000 Instagram posts from @unt to instagram_posts.csv.\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "# You code here (Please add comments in the code):\n",
    "\n",
    "import instaloader\n",
    "\n",
    "# Initialize Instaloader\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "# Define the target Instagram account (user_name or user_id)\n",
    "target_account = \"unt\"\n",
    "\n",
    "# Load the profile of the target account\n",
    "profile = instaloader.Profile.from_username(L.context, target_account)\n",
    "\n",
    "# Initialize a list to store the collected data\n",
    "posts_data = []\n",
    "\n",
    "# Collect posts\n",
    "for post in profile.get_posts():\n",
    "    # Get user_name, posted_time, and text\n",
    "    user_name = post.owner_username\n",
    "    posted_time = post.date\n",
    "    text = post.caption if post.caption else \"\"\n",
    "\n",
    "    # Append the data to the list\n",
    "    posts_data.append([user_name, posted_time, text])\n",
    "\n",
    "    # Break the loop once 1000 posts are collected\n",
    "    if len(posts_data) >= 1000:\n",
    "        break\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "import csv\n",
    "\n",
    "csv_file_name = 'instagram_posts.csv'\n",
    "with open(csv_file_name, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['User Name', 'Posted Time', 'Text'])\n",
    "    csv_writer.writerows(posts_data)\n",
    "    \n",
    "print(f'Collected and saved {len(posts_data)} Instagram posts from @{target_account} to {csv_file_name}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
