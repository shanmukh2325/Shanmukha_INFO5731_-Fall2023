{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW5_oFVd9-pY"
   },
   "source": [
    "## The second In-class-exercise (09/13/2023, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAzh1U0sE5I5"
   },
   "source": [
    "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
    "Execute all the cells before your final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpgvZQdRE-HV"
   },
   "source": [
    "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QBZI-je9-pZ"
   },
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWoKpYQT9-pa"
   },
   "source": [
    "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LmNR3kw9-pa"
   },
   "outputs": [],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "\n",
    "Research Question: What exactly are the most important elements influencing individual and family medical coverage cost rates?\" In order to resolve this issue, we would require data on a variety of criteria such as age, geography, coverage type, health problems, and more, as well as an analysis of how all of these variables affect insurance prices.\n",
    "\n",
    "Data Required: To address this issue, we would require a collection of data which contains the following sorts of data for a significant sample of people or families:\n",
    "\n",
    "Age: the covered person's or the eldest member of the insured family's age.\n",
    "Location: The insured's place of residence (for example, state or city).\n",
    "Coverage kind: The kind of medical insurance policy (individual, family, or employer-sponsored).\n",
    "Health issues: Data about any already present health issues.\n",
    "Deductibles are sums specified by the insured.\n",
    "Insurance Provider: The name of the insurer provider.\n",
    "The monthly or yearly payment paid for the protection coverage.\n",
    "Claim History: Information about previous filings for insurance. \n",
    "\n",
    "Sample Size: The amount of samples required will be determined by the degree of difficulty of the examination and the variance of the components. A random sample  of at least 1000 observations would be a good place to start for a full study. However, more data points would yield greater durability and accurate results.\n",
    "\n",
    "Steps for Collecting and Saving the Data:\n",
    "\n",
    "Data Source: Locate a trustworthy source of medical insurance data. This might be from insurance firms, organizations of government, or freely accessible databases.\n",
    "Data Collection: Gather the aforementioned data fields from a broad sample of insured people or households. Ascertain that the data is anonymised and that it conforms with privacy standards.\n",
    "Data Cleaning: Clean up the information by resolving missing numbers, outliers, and discrepancies. Ensure certain the data is in an organized format appropriate for analysis.\n",
    "Data Analysis: Conduct statistical analysis with Python or similar data analysis tool to investigate how various parameters (age, location, coverage type, and so forth) interact with premium rates.\n",
    "Data Visualization: Use visuals such as scatter plots, histograms, and heatmaps to illustrate correlations between variables.\n",
    "Save Results: Save the cleaned and analyzed data, as well as any visualizations and statistical results, to a CSV or Excel file for future reference and reporting.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlxTLRNm9-pa"
   },
   "source": [
    "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QpWOgjHi9-pa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic health insurance data with 1000 samples collected and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Initialize an empty list to store the data samples\n",
    "data_samples = []\n",
    "\n",
    "# Simulated data generation for 1000 samples\n",
    "for _ in range(1000):\n",
    "    sample = {\n",
    "        'Age': random.randint(18, 65),\n",
    "        'Location': random.choice(['NY', 'CA', 'TX', 'FL']),\n",
    "        'Coverage_Type': random.choice(['Individual', 'Family', 'Employer']),\n",
    "        'Health_Conditions': random.choice(['None', 'Diabetes', 'Heart Disease', 'Asthma']),\n",
    "        'Deductibles': random.choice([500, 1000, 250, 2000]),\n",
    "        'Insurance_Provider': random.choice(['Aetna', 'Blue Cross', 'Cigna', 'UnitedHealth']),\n",
    "        'Premium_Amount': random.randint(200, 1000),\n",
    "        'Claim_History': random.choice(['No Claims', 'Past Claims'])\n",
    "    }\n",
    "    \n",
    "    data_samples.append(sample)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data_samples)\n",
    "\n",
    "# Save the synthetic dataset to a CSV file\n",
    "df.to_csv('synthetic_health_insurance_data_1000.csv', index=False)\n",
    "\n",
    "print(\"Synthetic health insurance data with 1000 samples collected and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px6wgvog9-pa"
   },
   "source": [
    "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
    "\n",
    "The following information of the article needs to be collected:\n",
    "\n",
    "(1) Title\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: attrs>=20.1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium)\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.4 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "P5rjlclf9-pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n",
      "Scraped 660 articles and saved to acm_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Define the URL and headers\n",
    "base_url = \"https://dl.acm.org/action/doSearch\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Your User-Agent Header Here\",\n",
    "}\n",
    "\n",
    "# Initialize variables\n",
    "total_samples = 1000\n",
    "samples_collected = 0\n",
    "page_number = 1\n",
    "\n",
    "# Initialize a list to store the data\n",
    "article_data = []\n",
    "\n",
    "while samples_collected < total_samples:\n",
    "    # Define query parameters for the search\n",
    "    params = {\n",
    "        \"AllField\": \"data\",\n",
    "        \"expand\": \"all\",\n",
    "        \"ConceptID\": \"118230\",\n",
    "        \"pageNumber\": page_number,\n",
    "    }\n",
    "\n",
    "    # Send an HTTP GET request to the URL with query parameters\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the section with the article titles and authors\n",
    "        articles = soup.find_all('h5', class_='issue-item__title')\n",
    "\n",
    "        # Extract article titles and authors\n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            article_data.append({'Title': title})\n",
    "            samples_collected += 1\n",
    "\n",
    "        # Increment the page number for pagination\n",
    "        page_number += 1\n",
    "\n",
    "        # Break the loop if the desired number of samples is reached\n",
    "        if samples_collected >= total_samples:\n",
    "            break\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open('acm_articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(article_data)\n",
    "\n",
    "print(f\"Scraped {samples_collected} articles and saved to acm_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT3CNj_V9-pb"
   },
   "source": [
    "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
    "\n",
    "The following information needs to be collected:\n",
    "\n",
    "(1) User_name\n",
    "\n",
    "(2) Posted time\n",
    "\n",
    "(3) Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FymVNKVi9-pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected and saved 1000 Instagram posts from @unt_upc to instagram_posts.csv.\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "# You code here (Please add comments in the code):\n",
    "\n",
    "import instaloader\n",
    "\n",
    "# Initialize Instaloader\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "# Define the target Instagram account (user_name or user_id)\n",
    "target_account = \"unt\"\n",
    "\n",
    "# Load the profile of the target account\n",
    "profile = instaloader.Profile.from_username(L.context, target_account)\n",
    "\n",
    "# Initialize a list to store the collected data\n",
    "posts_data = []\n",
    "\n",
    "# Collect posts\n",
    "for post in profile.get_posts():\n",
    "    # Get user_name, posted_time, and text\n",
    "    user_name = post.owner_username\n",
    "    posted_time = post.date\n",
    "    text = post.caption if post.caption else \"\"\n",
    "\n",
    "    # Append the data to the list\n",
    "    posts_data.append([user_name, posted_time, text])\n",
    "\n",
    "    # Break the loop once 1000 posts are collected\n",
    "    if len(posts_data) >= 1000:\n",
    "        break\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "import csv\n",
    "\n",
    "csv_file_name = 'instagram_posts.csv'\n",
    "with open(csv_file_name, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['User Name', 'Posted Time', 'Text'])\n",
    "    csv_writer.writerows(posts_data)\n",
    "    \n",
    "print(f'Collected and saved {len(posts_data)} Instagram posts from @{target_username} to {csv_file_name}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
